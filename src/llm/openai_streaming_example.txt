{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}

{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}

....

{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}


The chat completion chunk object
Represents a streamed chunk of a chat completion response returned by model, based on the provided input.

id
string
A unique identifier for the chat completion. Each chunk has the same ID.
choices
array
A list of chat completion choices. Can contain more than one elements if n is greater than 1. Can also be empty for the last chunk if you set stream_options: {"include_usage": true}.

Hide properties
delta
object
A chat completion delta generated by streamed model responses.

Hide properties
content
string or null
The contents of the chunk message.
function_call
Deprecated
object
Deprecated and replaced by tool_calls. The name and arguments of a function that should be called, as generated by the model.

Show properties
tool_calls
array

Hide properties
index
integer
id
string
The ID of the tool call.
type
string
The type of the tool. Currently, only function is supported.
function
object

Hide properties
name
string
The name of the function to call.
arguments
string
The arguments to call the function with, as generated by the model in JSON format. Note that the model does not always generate valid JSON, and may hallucinate parameters not defined by your function schema. Validate the arguments in your code before calling your function.
role
string
The role of the author of this message.
refusal
string or null
The refusal message generated by the model.
logprobs
object or null
Log probability information for the choice.

Show properties
finish_reason
string or null
The reason the model stopped generating tokens. This will be stop if the model hit a natural stop point or a provided stop sequence, length if the maximum number of tokens specified in the request was reached, content_filter if content was omitted due to a flag from our content filters, tool_calls if the model called a tool, or function_call (deprecated) if the model called a function.
index
integer
The index of the choice in the list of choices.
created
integer
The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp.
model
string
The model to generate the completion.
service_tier
string or null
The service tier used for processing the request.
system_fingerprint
string
This fingerprint represents the backend configuration that the model runs with. Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.
object
string
The object type, which is always chat.completion.chunk.
usage
object or null
An optional field that will only be present when you set stream_options: {"include_usage": true} in your request. When present, it contains a null value except for the last chunk which contains the token usage statistics for the entire request.
